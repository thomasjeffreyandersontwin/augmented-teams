direct epric, story, acceptance criteraia read and write navigate, ROA exposed by MCP
move story io to real pythn tests

batch runs
Mural, somain oriented integrtatin first with drawio 

Asking a questin -- just answer dont do anythinhg
always trying to save legacy code


aito refresh MCP sever chach on update of code 

code stroy code to stroy bot uses cklean code (hack for now)


add fix action take in context, and inject "follow rule" when performking them

need simple runs

new base action - simple run 
story--> ac --> scenario -->  test --> code --> deploys

include rpe-exting work inproject to answer your questrions first

add cocneot of domakn sync and one knwledge graph across two 

confirm stroy scope of taks before strtting
eg explorarin cxonfoirmed tese arfe the stroies correct?

build a new bot congif and populate based on context

behavior add cuszotma citon

needto add architecture spike after shaping

scenario and examples merge and create strategy arou d embedded examples vs tables


synch and render need to be separate Tools

add the idea od ub behaviors (mayne aytinh with a 1_folder is a behavior?)

stories need an auto extractor back to json
strategies need to connect rulesyes
simp,le architecture temp,ating for code

connect test classes  to stories, tewst methosd to scenarios  and test files to epics in graph 
add a rule always plce helpers as locally as you can (test rule)

generator for test easy - creates test file, test classes and test scenario methods
will help kill nonsnense like 

# ============================================================================


# STORY: Generate Bot Tools (Increment 3)
# ============================================================================


givne when then tets one doicstring  > given when then comment above relevant code 

kill docstrings i hate them  

inject principls eg alays simplest code, least bespoke code, dont dsuppor tlegavcy


Add manual test to flow - gove steps reqiuried to manually test te systew,


validate state is not done when starting it again for a scoped item;  increment
way to eager to remove entire tests

fix disocvered AC are marked as found during testing vs up front


help for user testing  --> copuld be generation of comand that have steps AI can do iwth some data

links ion story md to code

more deliberatelymoving workflow out of the instructions so that no workflow happens if you drag and drop something inYou have to be specific as it's confusing the actual code, that all we only include it if we're not using codes
we need the concepts of runs because a workflow only makes sense within the concept of a run EG we might be doing a workflow on a particular single story while also doing a run on a particular epicSo we need to be able to set the run and then go back to that run
all render documents should have hyperlinks to other render documentsas appropriate


standsalone synch command 



make instructions a lot for the actual bot that's running where you are getting into instruction below again which is the reason I started this in the first place is to avoid this kind of thing
 









------------------
call a behaviorCall all behaviors with one action
need something to in the background just update everything based on what has changed the latest so if the story graph was changed it just runs render automatically or if an artifact was changed it renders extract and then render




QualityTradeoffScanner - apply_quality_tradeoffs_for_minimal_spine.json
Check minimal spine vs enhanced features distinction
Verify quality tradeoffs are explicit


no


ArchiveNotDeleteScanner - archive_not_delete.json
Check obsolete files are archived, not deleted
Verify archive folder structure

no

ExhaustiveDecompositionScanner - apply_exhaustive_decomposition.json (already exists, verify)
Check all stories are decomposed
Verify no "~X stories" notation in focus increment
StoryEnumerationScanner - enumerate_all_stories_explicitly.json
Check focus increment lists all stories explicitly
Detect "~X stories" notation violations
Verify story expansion based on approach


merge


ConsolidationReviewScanner - present_consolidation_review.json
Check consolidation opportunities are identified
Verify review format no


StoryExpansionScanner - review_and_expand_stories.json
Check stories are expanded based on new approaches
Verify component-interaction stories are created

no

Scanners needed:
AcceptanceCriteriaScanner - enumerate_all_ac_permutations.json
Check all AC permutations are enumerated
Verify scenario coverage


how?


ScenarioCoverageScanner - scenarios_cover_all_cases.json (reuse from scenarios)
Verify scenarios cover all AC cases

how? and beon gto scenario


ackgroundSetupScanner - use_background_for_common_setup.json (reuse from scenarios)
Check common setup in Background section
ScenarioOutlineScanner - use_scenario_outline_when_needed.json
Check Scenario Outlines used for multiple similar scenarios
Detect when outlines should be used

belong to snario not ecxploration


ScenarioCoverageScanner - scenarios_cover_all_cases.json
Check scenarios cover all AC cases
Verify edge cases covered


how


MockBoundariesScanner - mock_only_boundaries.json (adapt BDDLayerFocusHeuristic)
Detect mocking of internal code
Check mocks are only for external boundaries
Verify real implementations used for internal code

all mocks must be mentroned in planing criteria json or are fornbiddem

ProductionCodeDirectScanner - call_production_code_directly.json
Detect commented-out code
Check production code is called directly
Verify no test stubs
merge with
RealImplementationsScanner - use_real_implementations.json
Detect fake/stub implementations
Check real implementations are used

and 

ensure we arent makn test that erify helper code MUST call api even if API doesnt exists

ExactVariableNamesScanner - use_exact_variable_names.json
Check variable names match scenario/AC/domain model concepts exactly
Verify consistent vocabulary


MatchSpecificationScanner - match_specification_scenarios.json
Check tests match specification scenarios exactly
Verify 1-to-1 mapping


BugFixTestFirstScanner - bug_fix_test_first.json
Check bug fixes have tests first
Verify TDD approach

how>







belong in code as well 
ProductionCodeAPIScanner - production_code_api_design.json
Check production code API design from tests
Verify API emerges from test needs
ProductionCodeDependenciesScanner - production_code_explicit_dependencies.json
Check explicit dependencies in production code
Detect hidden dependencies
ProductionCodeSRScanner - production_code_single_responsibility.json
Check single responsibility in production code
Detect multiple responsibilities
ProductionCodeSmallFunctionsScanner - production_code_small_functions.json
Check function size limits
Detect large functions

SelfDocumentingTestsScanner - self_documenting_tests.json
Check tests are self-documenting
Detect need for comments--> remove all ai geenrstged dosctrings i hate those!


move to test
OneConceptPerTestScanner - test_one_concept_per_test.json
Check one concept per test
Detect multiple concepts


UselessCommentsScanner - stop_writing_useless_comments.json
Detect useless comments
Check comments add value
kill ai gen comments 

merge with |
PreferCodeOverCommentsScanner - prefer_code_over_comments.json
Check code is self-documenting
Detect comment overuse
and 
RemoveBadCommentsScanner - remove_bad_comments.json
Detect bad comments
Check comment remova



RefactorTestsScanner - refactor_tests_with_production_code.json
Check tests are refactored with production code
Verify refactoring approac
how?

TDDScanner - practice_test_driven_development.json
Check TDD is practiced
Verify test-first approach how?


FormattingConsensusScanner - enforce_team_formatting_consensus.json
Check formatting consistency
Verify team standards

kill



NOBackwardCompatibilityScanner - handle_backward_compatibility.json
remove code vcreate for basckward compat purpose
update all code to new internface /code


GoodCommentsScanner - write_good_comments.json
Check comment quality
Verify useful comments

kill