---
description: Code agent behavior structure and naming conventions
globs:
  - "**/*-rule.mdc"
  - "**/*-cmd.md"
  - "**/*-runner.py"
  - "**/behavior.json"
  - "**/.cursor/rules/**"
  - "**/.cursor/commands/**"
---

**When** creating or updating Code Agent behavior
**then** follow the defined structure, standards, and naming conventions in order to amximize modularity, consistency and discoverability.

**Code Agent Behavior**
* is a reusable instruction set that defines how the AI assistant should respond to specific situations, commands, or contexts. 
* Each behavior consists of one or more files that work together: 
  * a **rule** (`.mdc`) that defines the triggering condition and guidelines, 
  * one or more **commands** (`.md`) that describe how to execute the behavior
* may optionally include
  * a **Python implementation** (`.py`) that provides automated functionality. 
  * may also include **MCP tool configurations** (`.json` files) for exposing tools via the Model Context Protocol
  * may a **task configurations** (`.vscode/tasks.json`) for background processes or automation workflows. 

**Runner Location:**
* ALL runner files (`.py`) MUST stay in their feature directory: `behaviors/<feature>/`
* Commands and documentation MUST reference runners using local path: `python behaviors/<feature>/<runner-name>.py`
* NEVER copy runners to `command-runners/` directory
* NEVER reference `command-runners/` in documentation or commands
* Each feature owns and maintains its own runner files 

**Code Agent Behavior Features**
Behaviors are organized within a **Behavior Features** —  a collection of tightly nit behaviors that are strongly related.

**Always**
* Named according to the overarching purpose of the feature 
* Provide a brief overview of the main behaviors and purpose of the feature
* Keep behavior files belonging to the same feature together in one directory 
* Name rule files using `<feature-name>-<behavior-name>-rule.mdc`
* Name Python runner files using consolidated pattern:
  - **Feature-level runner:** `<feature>-runner.py` (contains all feature behaviors)
  - **Behavior-level runner:** `<feature>-<behavior>-runner.py` (for complex individual behaviors)
  - Runners include `require_command_invocation()` guard function internally
  - Call via: `python behaviors/<feature>/<feature>-runner.py <command> [args]`
* Name MCP tool configurations: `<feature>-<tool-name>-mcp.json`
* Name Task configurations: `<feature>-tasks.json`
* Keep each behavior in its own feature folder eg `behaviors/code-agent/`
* Link related files by prefix (same `<feature>-<behavior-name>` prefix; commands can have additional verb suffixes)

**Never**
* Scatter behaviors into separated areas areas, break them up by component type.
* Mix files from different features in the same folder.
* Use inconsistent or ad hoc filename patterns.
* Add undocumented, draft, or experimental files to the deployed behaviors directory.
* Manually edit auto-generated index or sync files.
* Place tool configuration or task files outside their designated folders.

**Rule Files:**
*  define the overarching conditions and guidelines for when and how a behavior should be applied. 
*  focus on both overarching logic, structural standards, and event-based behavior patterns.

**Always:**
*start  Rules with `**When** <event> condition,` — Describe the triggering event or condition
* Follow with `**then** <action>` — Describe what should happen
* Have a structure, logic, or applicable conditions section
* Rules file must reference the commands that can execute their rules — Include section: `**Executing Commands:** * \command-name — [what the command is] [when to call it]`
* Rules must reference tools used by the behavior** — If the behavior uses MCP servers or tools, include section: `**Tools:** * @mcp-server-name — [list all tools from the MCP server and how the behavior uses them]` (e.g., if using TDD MCP server, list `execute_next_step`, `get_current_step` and how they're used)

**Never:**
* Reference commands in rules that do not exist, or vice versa.


**Command Files:**
* contain a set of executable steps that implement a rule. 
* describe the concrete actions, procedures, and workflows that the AI agent, code functions, or user should perform to execute the behavior. 
* bridge the gap between abstract rules (the "what") and concrete implementation (the "how").

**Always:**
* Commands must include (DRY pattern):
  * `**Rule:**` — Which rule this command follows (one-liner: rule name and why)
  * `**Runner:**` — Which runner file and why (one-liner, if applicable): `python behaviors/<feature>/<runner>.py <command>`
  * `**MCP Servers:**` — Comma-listed servers used, what and why (one-liner, if applicable): `@server-name (purpose)`
  * `**Steps:**` — Sequential actions with clear performer specification:
    - Runner function calls: Include function signature and return value
    - MCP tool calls: Include tool name, parameters, and return value
    - AI Agent actions: Specify what analysis is performed
    - User actions: Specify what input/interaction is required
* Document relationships (rule ↔ command ↔ runner) using references
* Include a short description at the top of every rule and command file
* Validate structure before committing changes
* Version all behavior files via Git; treat each change as a traceable commit

**Never:**
* Include `**AI Usage:**` or `**Code Usage:**` sections (redundant with Steps)
* Include `**Implementation:**` section (replaced by Runner reference)
* Mix unrelated behaviors in the same folder
* Use ambiguous names like `new-rule.mdc` or `temp-cmd.md`
* Leave files undocumented or unreferenced
* Create rules without referencing executing commands
* Create commands without referencing rules and runners
* Manually fix structure issues that can be automated

**Command Steps:**
Commands must include a `**Steps:**` section that lists the sequential actions to execute the behavior. Each step MUST specify who/what performs it:
* **User steps** — User actions (e.g., "1. **User** invokes command via `/command-name`")
* **Code steps** — Python functions that execute deterministic tasks
  - MUST include function signature with parameters (e.g., "2. **Code** (`Commands.structure(action="validate", feature)`) calls `Feature.validate(feature)` to discover and validate features")
  - MUST specify the actual function name and key parameters
  - MUST indicate what the function returns or outputs
* **AI Agent steps** — AI (in conversation) analyzes data and makes judgments (e.g., "3. **AI Agent** evaluates test file against BDD principles")
  - Can be the AI assistant analyzing function output directly in conversation
  - OR can use **OpenAI GPT function calling** (must be explicitly mentioned: "**AI Agent** (via GPT function calling) evaluates...")
* **MCP tool steps** — Calls to MCP servers with tool name and parameters (e.g., "4. **MCP Tool** `execute_next_step(feature_name)` is called to advance TDD cycle")

**Command Runner Implementation:**
* **Consolidated runners** contain multiple related behavior functions in a single file
* Each **Code step** is a Python function that performs deterministic operations (file I/O, parsing, data extraction, etc.)
* Each **AI Agent step** is a Python function that uses OpenAI GPT function calling for semantic analysis:
  - Uses `from openai import OpenAI`
  - Loads `OPENAI_API_KEY` from `.env` file using `python-dotenv`
  - Defines function schema for structured output
  - Calls `client.chat.completions.create()` with `functions` and `function_call` parameters
  - Returns parsed structured results from AI
* Main orchestrator function calls step functions in sequence
* Code steps prepare data, AI steps perform semantic judgment, code steps process results
* **ALL runners include `require_command_invocation()` guard function:**
  - Built into the consolidated runner file
  - Called in main dispatcher before executing commands
  - Ensures runners are only called via Cursor slash commands with `--from-command` flag
  - Prevents bypassing command orchestration and AI workflow

**Example Pattern:**
```python
# Step 2: Code function (deterministic operations)
def load_rules_and_extract_examples(file_path):
    """Code step: Load rule file and extract DO/DON'T examples."""
    framework = detect_framework(file_path)
    rule_content = load_rule_file(framework)
    return extract_dos_and_donts(rule_content)

# Step 3: AI Agent function (Python function using OpenAI function calling)
def evaluate_with_ai(test_content, dos_and_donts):
    """AI Agent step: Semantic evaluation using OpenAI GPT."""
    from openai import OpenAI
    from dotenv import load_dotenv
    import os
    
    # Load API key from .env
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        return {"error": "OPENAI_API_KEY not set in .env"}
    
    client = OpenAI(api_key=api_key)
    
    # Define function schema for structured output
    VALIDATION_SCHEMA = {
        "name": "validate_test",
        "description": "Validate test against BDD principles",
        "parameters": {
            "type": "object",
            "properties": {
                "violations": {"type": "array", "items": {...}},
                "passes": {"type": "array", "items": {...}}
            }
        }
    }
    
    # Call OpenAI with function calling
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are an expert..."},
            {"role": "user", "content": prompt}
        ],
        functions=[VALIDATION_SCHEMA],
        function_call={"name": "validate_test"}
    )
    
    # Parse and return structured results
    if response.choices[0].message.function_call:
        return json.loads(response.choices[0].message.function_call.arguments)
    return {"error": "No function call result"}

# Step 4: Code function (process results)
def generate_report(results):
    """Code step: Format validation results into report."""
    ...

# Main orchestrator
def command_runner(file_path):
    # Step 2: Code - prepare data
    examples = load_rules_and_extract_examples(file_path)
    
    # Step 3: AI Agent - semantic analysis via OpenAI
    results = evaluate_with_ai(test_content, examples)
    
    # Step 4: Code - process and report
    report = generate_report(results)
    return report

# Entry point with guard
if __name__ == "__main__":
    import sys
    from runner_guard import require_command_invocation
    
    # GUARD: Ensure runner is called from Cursor command, not directly
    require_command_invocation("my-command")
    
    # Process arguments and run
    file_path = sys.argv[1] if len(sys.argv) > 1 else None
    command_runner(file_path)
```

---

## Advanced Patterns

For behaviors requiring framework-specific, language-specific, or context-specific variations, see `code-agent-specialization-rule.mdc` for hierarchical rule patterns that maintain DRY principles while supporting specialized implementations.



For behaviors requiring framework-specific, language-specific, or context-specific variations, see `code-agent-specialization-rule.mdc` for hierarchical rule patterns that maintain DRY principles while supporting specialized implementations.

