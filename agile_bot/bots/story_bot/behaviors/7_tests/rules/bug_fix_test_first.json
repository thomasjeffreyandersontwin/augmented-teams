{
  "description": "When production code breaks, ALWAYS follow the test-first workflow: write/modify failing test, verify failure, fix code, verify success, then test in production. Never fix bugs directly without a failing test first.",
  "rationale": [
    "Writing a failing test before fixing a bug ensures: (1) we can reproduce the bug, (2) we know when it's truly fixed, (3) we prevent regression, (4) we document the bug for future reference.",
    "Skipping the failing test step means we might fix the wrong thing or not really fix it at all.",
    "A failing test proves we understand the bug and can reproduce it. Without this, we're guessing at the fix.",
    "If the test doesn't fail, either: (a) we're testing the wrong thing, (b) the bug doesn't exist in test environment, (c) the test is wrong. We must know the test fails before proceeding.",
    "Minimal fixes reduce risk of introducing new bugs. If more changes are needed, they should be separate commits with their own tests.",
    "A fix in one area might break something else. Running the full test suite catches regressions immediately.",
    "Production testing is expensive (time, risk). Never waste time testing in production until automated tests confirm the fix works.",
    "Tests that pass but don't prevent production failures are worse than no tests - they give false confidence. The test must be improved to catch what production caught.",
    "Tests serve as documentation. Future developers should understand what bug was fixed by reading the test."
  ],
  "examples": [
    {
      "do": {
        "description": "Follow RED-GREEN-PRODUCTION workflow",
        "content": [
          "# Bug: MCP tool returns 'Bot not initialized' error",
          "",
          "# Step 1: RED - Write/modify test to reproduce bug",
          "def test_mcp_tool_initializes_bot_before_invocation(self, workspace_root):",
          "    \"\"\"",
          "    SCENARIO: MCP tool initializes bot before invocation",
          "    GIVEN: MCP server is started",
          "    WHEN: Tool is invoked",
          "    THEN: Bot is initialized and tool executes successfully",
          "    \"\"\"",
          "    # Given: MCP server setup",
          "    from agile_bot.bots.base_bot.src.mcp.server import MCPServer",
          "    server = MCPServer(workspace_root=workspace_root)",
          "    ",
          "    # When: Invoke tool",
          "    result = server.invoke_tool('test_bot_shape_gather_context', {})",
          "    ",
          "    # Then: Bot initialized and tool executed",
          "    assert result.status == 'completed'",
          "    assert server.bot is not None",
          "",
          "# Step 2: Run test - VERIFY it fails with 'Bot not initialized'",
          "# pytest output: AttributeError: 'NoneType' object has no attribute 'invoke'",
          "",
          "# Step 3: GREEN - Make minimal fix",
          "# In mcp_server_generator.py:",
          "#   self.bot = Bot(bot_name=bot_name, workspace_root=workspace_root)",
          "",
          "# Step 4: Run test - VERIFY it passes",
          "",
          "# Step 5: Run full test suite - verify no regressions",
          "",
          "# Step 6: PRODUCTION - Test in real MCP server environment"
        ]
      },
      "dont": {
        "description": "Fix first, test later",
        "content": [
          "# DON'T: Fix bug directly without test",
          "# Bug: MCP tool returns 'Bot not initialized' error",
          "",
          "# WRONG: Directly edit mcp_server_generator.py",
          "#   self.bot = Bot(...)  # Hope this fixes it",
          "",
          "# WRONG: Restart server and test in production",
          "#   - Restart MCP server",
          "#   - Test tool invocation",
          "#   - Still fails? Edit again",
          "#   - Repeat 3-5 times",
          "",
          "# Problems:",
          "# - Can't prove the fix actually solves the problem",
          "# - Might fix wrong thing",
          "# - No way to verify it stays fixed",
          "# - No automated test to prevent regression"
        ]
      }
    },
    {
      "do": {
        "description": "Verify test fails before fixing",
        "content": [
          "# Step 1: Write test",
          "def test_bot_initializes_correctly(self, workspace_root):",
          "    bot = Bot(bot_name='test_bot', workspace_root=workspace_root)",
          "    assert bot.is_initialized",
          "",
          "# Step 2: Run test - MUST FAIL",
          "# pytest output: AssertionError: assert False == True",
          "# GOOD: Test fails as expected",
          "",
          "# Step 3: Now fix the code",
          "# In bot.py:",
          "#   def __init__(self, ...):",
          "#       self._initialized = True  # Fix",
          "",
          "# Step 4: Run test - should pass now"
        ]
      },
      "dont": {
        "description": "Skip verifying test fails",
        "content": [
          "# DON'T: Write test and immediately fix",
          "def test_bot_initializes(self, workspace_root):",
          "    bot = Bot(bot_name='test_bot', workspace_root=workspace_root)",
          "    assert bot.is_initialized",
          "",
          "# WRONG: Immediately add fix without running test first",
          "# In bot.py:",
          "#   self._initialized = True",
          "",
          "# WRONG: Run test - passes",
          "# Deploy to production - still fails!",
          "# Why? Test was testing wrong thing or bug doesn't exist in test environment",
          "",
          "# Problem: Can't tell if fix worked or test was wrong"
        ]
      }
    },
    {
      "do": {
        "description": "Make minimal fix, run full test suite",
        "content": [
          "# Bug: Bot not initialized",
          "",
          "# Step 1: Test fails (verified)",
          "",
          "# Step 2: Make MINIMAL fix",
          "# In mcp_server_generator.py:",
          "#   def __init__(self, ...):",
          "#       # MINIMAL: Only add bot initialization",
          "#       self.bot = Bot(bot_name=bot_name, workspace_root=workspace_root)",
          "",
          "# Step 3: Run the failing test - passes",
          "",
          "# Step 4: Run ALL related tests",
          "#   pytest test_mcp_server.py",
          "#   pytest test_bot_initialization.py",
          "#   pytest test_tool_invocation.py",
          "#   All 30 tests pass - good!",
          "",
          "# Step 5: Test in production"
        ]
      },
      "dont": {
        "description": "Make large changes while fixing bug",
        "content": [
          "# DON'T: Mix bug fix with refactoring",
          "# Bug: Bot not initialized",
          "",
          "# WRONG: While fixing, also refactor entire class",
          "# In mcp_server_generator.py:",
          "#   def __init__(self, ...):",
          "#       self.bot = Bot(...)  # Bug fix",
          "#       # But also:",
          "#       self._refactor_entire_class()  # WRONG!",
          "#       self._rename_all_variables()  # WRONG!",
          "#       self._add_new_features()  # WRONG!",
          "",
          "# Problems:",
          "# - Can't tell which change fixed the bug",
          "# - Might have introduced new bugs",
          "# - Mixing concerns makes debugging harder"
        ]
      }
    },
    {
      "do": {
        "description": "Improve test if production still fails",
        "content": [
          "# Bug: UTF-8 encoding error in production",
          "",
          "# Step 1: Write test",
          "def test_json_writes_with_utf8_encoding(self, tmp_path):",
          "    file_path = tmp_path / 'data.json'",
          "    data = {'text': 'Hello 世界'}",
          "    write_json(file_path, data)",
          "    ",
          "    content = file_path.read_text(encoding='utf-8')",
          "    assert '世界' in content",
          "",
          "# Step 2: Test passes (but production still fails)",
          "",
          "# Step 3: Improve test to catch real issue",
          "def test_json_writes_with_utf8_encoding(self, tmp_path):",
          "    file_path = tmp_path / 'data.json'",
          "    data = {'text': 'Hello 世界'}",
          "    ",
          "    # Test actual write operation",
          "    write_json(file_path, data)",
          "    ",
          "    # Verify encoding is correct",
          "    content = file_path.read_bytes()",
          "    assert content.decode('utf-8') == json.dumps(data)",
          "",
          "# Step 4: Test now fails - good!",
          "",
          "# Step 5: Fix encoding in write_json()",
          "#   file_path.write_text(json.dumps(data), encoding='utf-8')",
          "",
          "# Step 6: Test passes, production works"
        ]
      },
      "dont": {
        "description": "Fix code without improving test",
        "content": [
          "# DON'T: Fix code when test passes but production fails",
          "# Bug: UTF-8 encoding error in production",
          "",
          "# WRONG: Test passes but doesn't catch encoding issue",
          "def test_json_writes(self, tmp_path):",
          "    file_path = tmp_path / 'data.json'",
          "    write_json(file_path, {'text': 'Hello'})",
          "    assert file_path.exists()  # Too simple!",
          "",
          "# WRONG: Fix encoding in code",
          "#   file_path.write_text(json.dumps(data), encoding='utf-8')",
          "",
          "# WRONG: Test still passes (wasn't checking encoding)",
          "# Deploy - production still fails!",
          "",
          "# Problem: Test doesn't catch what production caught"
        ]
      }
    },
    {
      "do": {
        "description": "Document bug in test name and scenario",
        "content": [
          "# GOOD: Test name documents the bug",
          "def test_server_initializes_bot_instance_to_prevent_not_initialized_error(self, workspace_root):",
          "    \"\"\"",
          "    SCENARIO: Server initializes bot instance to prevent 'not initialized' error",
          "    GIVEN: MCP server is created",
          "    WHEN: Server starts",
          "    THEN: Bot instance is initialized",
          "    AND: Tool invocations don't fail with 'Bot not initialized' error",
          "    \"\"\"",
          "    # Test implementation...",
          "",
          "# GOOD: Future developers understand:",
          "# - What bug was fixed (not initialized error)",
          "# - How it was fixed (bot initialization)",
          "# - Why it matters (prevents tool invocation failures)"
        ]
      },
      "dont": {
        "description": "Use generic test names",
        "content": [
          "# DON'T: Generic test name",
          "def test_server_works(self, workspace_root):",
          "    \"\"\"",
          "    SCENARIO: Server works",
          "    \"\"\"",
          "    # Test implementation...",
          "",
          "# BAD: Future developers can't tell:",
          "# - What bug was this fixing?",
          "# - What does 'works' mean?",
          "# - Why does this test exist?"
        ]
      }
    }
  ],
  "key_principles": [
    "WHEN production code breaks, ALWAYS write or modify a test to reproduce the bug FIRST",
    "ALWAYS run the test BEFORE fixing to confirm it fails for the RIGHT reason",
    "Make the MINIMAL change to fix the bug - just enough to make the test pass",
    "Run ALL related tests after the fix to ensure no regression",
    "ONLY AFTER tests pass, test the fix in production/real environment",
    "If fix works in tests but fails in production, the test is INCOMPLETE - return to step 1",
    "Document the bug and fix in the test name and test scenario",
    "RED: Write/modify test to reproduce bug (test fails)",
    "GREEN: Make minimal fix to pass the test",
    "PRODUCTION: Test in real environment only after automated tests pass"
  ],
  "antipatterns": [
    "Fix first, test later - Can't prove the fix actually solves the problem. Might fix wrong thing. No way to verify it stays fixed.",
    "Skip verifying test fails before fixing - Test might be wrong or testing the wrong thing. You'll make unnecessary changes or miss the real bug.",
    "Test only in production - Wastes time (restart servers, wait for changes). Risky (might break production). No automation (will break again).",
    "Make large changes while fixing bug - Can't tell if the bug fix worked or if new code introduced new bugs. Mixing refactoring with bug fixes leads to confusion.",
    "Fix code without improving test when production still fails - Tests that pass but don't prevent production failures are worse than no tests - they give false confidence.",
    "Use generic test names - Future developers can't understand what bug was fixed by reading the test"
  ],
  "workflow": [
    "1. RED: Write/modify test to reproduce bug (test fails)",
    "2. Verify test fails for the right reason",
    "3. GREEN: Make minimal fix to pass the test",
    "4. Verify test now passes",
    "5. Run full test suite (verify no regression)",
    "6. PRODUCTION: Test in real environment (MCP server, actual usage)",
    "7. If production still fails: repeat from step 1 with better test"
  ],
  "checklist": [
    "[ ] Written or modified test to reproduce the bug",
    "[ ] Run test - VERIFIED it fails",
    "[ ] Confirmed test fails for the RIGHT REASON (not a test bug)",
    "[ ] Made MINIMAL fix to address the bug",
    "[ ] Run test - VERIFIED it now passes",
    "[ ] Run FULL test suite - VERIFIED no regressions",
    "[ ] Test in production/real environment",
    "[ ] If production still fails, repeat from step 1 with better test"
  ],
  "real_world_example": {
    "bug": "MCP tool returns 'Bot not initialized' error in production",
    "workflow_followed": [
      "1. RED: Saw existing test test_bot_tool_invocation.py exists but uses mocks",
      "2. RED: Modified test to use real MCPServerGenerator",
      "3. RED: Ran test - FAILED with 'Bot not initialized' (good!)",
      "4. GREEN: Added self.bot = Bot(...) to mcp_server_generator.py",
      "5. GREEN: Ran test - PASSED",
      "6. Run all 30 tests - ALL PASSED",
      "7. PRODUCTION: Restarted MCP server, tested - SUCCESS",
      "8. Found UTF-8 encoding bug in production",
      "9. RED: Test helper writes JSON without encoding",
      "10. GREEN: Added encoding='utf-8' to all JSON writes",
      "11. Run all tests - PASSED",
      "12. PRODUCTION: Restarted MCP server, tested - SUCCESS"
    ],
    "outcome": "Bug fixed correctly, with automated tests preventing regression, and caught additional bug (UTF-8) through the process"
  }
}
